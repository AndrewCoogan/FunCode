{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from os.path import exists\n",
    "import itertools\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Make sure you have a folder /Models in your running location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/adult.data', header=None, skipinitialspace=True)\n",
    "data.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\\\n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\\\n",
    "                'native-country', 'income']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values are denoted as '?' in the data.\n",
    "# We can see there are a few columns that are missing data.\n",
    "# I am going to try two different approaches for dealing with this later on.\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the data look like and how do we want it to look?\n",
    "\n",
    "Age -- Continuous, no missing values\n",
    "\n",
    "Workclass -- Categorical, what sector is their employment in.  A few missing values, but we can probably bucketize this.\n",
    "\n",
    "fnlweight -- Continuous, this mertic explains how common the line item is for the nation.  I think this should be removed, but will test both.\n",
    "\n",
    "Education -- Categorical, will remove and use Education-Num.\n",
    "\n",
    "Education-Num -- Continuous, same info as education.\n",
    "\n",
    "Marital-Status -- Categorical, no missing values.\n",
    "\n",
    "Occupation -- Categorical, some missing values.  This is going to be a lot of unique values, I think this will be punted and workclass used.\n",
    "\n",
    "Relationship -- Categorical, may look to reduce dimenstionality for this one.\n",
    "\n",
    "Race -- Categorical, pretty concentrated, will probably dum-ify these.\n",
    "\n",
    "Sex -- Categorical, two options\n",
    "\n",
    "Capital-gain -- Continuous, mostly 0, we can possibly change this to be a bool if greater than zero.\n",
    "\n",
    "Capital-loss -- Continuous, mostly 0, we can possibly change this to be a bool if greater than zero.\n",
    "\n",
    "Hours-per-week -- Continuous\n",
    "\n",
    "Native-country -- Categorical, missing a few values.  Mostly concentraded as US, will summarize.\n",
    "\n",
    "Income -- Categorical, target value, what we are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I suspect there is a good chunk of overlap here.\n",
    "data['workclass'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to dummy this, but this will control some of the dimensionality.\n",
    "### NOTE: We are overwriting the NAN's and setting them to 3 (other) we should make note of this for when we test.\n",
    "gov_jobs = ['Local-gov', 'State-gov', 'Federal-gov']\n",
    "self_emp_jobs = ['Self-emp-not-inc', 'Self-emp-inc']\n",
    "\n",
    "data['workclass-cat'] = 3\n",
    "data.loc[data['workclass'].isin(self_emp_jobs), 'workclass-cat'] = 2\n",
    "data.loc[data['workclass'].isin(gov_jobs), 'workclass-cat'] = 1\n",
    "data.loc[data['workclass'] == 'Private', 'workclass-cat'] = 0\n",
    "\n",
    "data['workclass-cat'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education and Education Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at education, we can boot this.  Education-Num is inclusive enough.\n",
    "data[['education', 'education-num']].groupby('education').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marital Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['marital-status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['relationship'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race and Native Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can honestly make this into 'White', 'Black', 'Other'\n",
    "data['race'].value_counts(normalize=True)\n",
    "\n",
    "data['race-cat'] = 2\n",
    "data.loc[data['race'] == 'Black', 'race-cat'] = 1\n",
    "data.loc[data['race'] == 'White', 'race-cat'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['native-country'].value_counts(normalize=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since a vast majority come from the US, and everything else is less than 2%, lets make a new column that is US Bool\n",
    "data['from-US'] = 0\n",
    "data.loc[data['native-country'] == 'United-States', 'from-US'] = 1\n",
    "data['from-US'].astype('object')\n",
    "data['from-US'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should convert sex from string to binary.  Easy enough.\n",
    "data['sex'] = data['sex'].map({'Male':1,'Female':0}).astype(object)\n",
    "data['sex'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitol Gain and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['capital-gain'] > 0, 'capital-gain'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['capital-loss'] > 0, 'capital-loss'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Couple Graphs to Demonstrate Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_by_education = data[['income', 'education-num']].groupby('income')\n",
    "income_by_education.boxplot()\n",
    "plt.suptitle('Education vs Income')\n",
    "plt.show()\n",
    "# Not surprisingly, we can see that those who make >50k a year have more years of education\n",
    "# I kinda want to expand this a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=data[['age','income']], x='age', hue='income',\\\n",
    "            cumulative=True, common_norm=False, common_grid=True).set(title='CDF Income vs Age')\n",
    "# There is a big age gap here, which is also intuitive.\n",
    "# People are not going to make more than 50k in their early years of productivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=data[['hours-per-week','income']], x='hours-per-week', hue='income',\\\n",
    "            cumulative=True, common_norm=False, common_grid=True).set(title='CDF Income vs Hours Worked Per Week')\n",
    "# This is also intuitive, those who make more than 50k a year end up working more hours on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Before We Start Modeling\n",
    "\n",
    "Columns to remove:\n",
    "    - workclass, fnlwgt, education, occupation, race, capital-gain, capital-loss\n",
    "    \n",
    "Columns to dummy:\n",
    "    - marital-status, relationship, race-cat, workclass-cat, race-cat\n",
    "    \n",
    "Things to modify:\n",
    "    - change income to 1 if '>50K', else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['workclass', 'fnlwgt', 'education', 'occupation', 'race', 'native-country']\n",
    "cols_to_dummy = ['marital-status', 'relationship', 'race-cat', 'workclass-cat']\n",
    "\n",
    "data['income'] = data['income'].map({'>50K':1,'<=50K':0}).astype(int)\n",
    "data.drop(cols_to_drop, inplace=True, axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(data[cols_to_dummy])\n",
    "data[dummies.columns] = dummies\n",
    "data.drop(cols_to_dummy, inplace=True, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data() -> pd.DataFrame:\n",
    "    data = pd.read_csv('Data/adult.data', header=None, skipinitialspace=True)\n",
    "    data.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\\\n",
    "                    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\\\n",
    "                    'native-country', 'income']\n",
    "    \n",
    "    # Change the missing value (?) to something we can work with.\n",
    "    data.replace('?', np.nan, inplace=True)\n",
    "    \n",
    "    # We are going to bucketize the workclass\n",
    "    gov_jobs = ['Local-gov', 'State-gov', 'Federal-gov']\n",
    "    self_emp_jobs = ['Self-emp-not-inc', 'Self-emp-inc']\n",
    "\n",
    "    data['workclass-cat'] = 3\n",
    "    data.loc[data['workclass'].isin(self_emp_jobs), 'workclass-cat'] = 2\n",
    "    data.loc[data['workclass'].isin(gov_jobs), 'workclass-cat'] = 1\n",
    "    data.loc[data['workclass'] == 'Private', 'workclass-cat'] = 0\n",
    "    \n",
    "    # We are going to bucketize the races\n",
    "    data['race-cat'] = 2\n",
    "    data.loc[data['race'] == 'Black', 'race-cat'] = 1\n",
    "    data.loc[data['race'] == 'White', 'race-cat'] = 0\n",
    "    \n",
    "    # And where everyone is from\n",
    "    data['from-US'] = 0\n",
    "    data.loc[data['native-country'] == 'United-States', 'from-US'] = 1\n",
    "    \n",
    "    # Map male and female to numerical\n",
    "    data['sex'] = data['sex'].map({'Male':1,'Female':0})\n",
    "    \n",
    "    # Map the target column\n",
    "    data['income'] = data['income'].map({'>50K':1,'<=50K':0})\n",
    "    \n",
    "    cols_to_drop = ['workclass', 'fnlwgt', 'education', 'occupation', 'race', 'native-country']    \n",
    "    data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_model(model, name):\n",
    "    with open(f'Models/{name}.pkl', 'wb') as fid:\n",
    "        pickle.dump(model, fid)  \n",
    "        \n",
    "def load_model(name):\n",
    "    with open(f'Models/{name}.pkl', 'rb') as fid:\n",
    "        model = pickle.load(fid)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Make a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_and_process_data()\n",
    "\n",
    "TGT_COL = ['income']\n",
    "NUM_COL = ['age', 'education-num', 'hours-per-week', 'capital-gain', 'capital-loss']\n",
    "CAT_COL = ['marital-status', 'relationship', 'race-cat', 'workclass-cat']\n",
    "\n",
    "for cc in CAT_COL:\n",
    "    data[cc] = data[cc].astype('object')\n",
    "\n",
    "dummies = pd.get_dummies(data[CAT_COL]) # Get Dummies only works on categorical columns!\n",
    "data = pd.concat([data, dummies], axis=1)\n",
    "data.drop(CAT_COL, inplace=True, axis=1)\n",
    "  \n",
    "X = data.drop('income', axis=1)\n",
    "y = data['income']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "sc_num_cols = pd.DataFrame(ss.fit_transform(X_train[NUM_COL]),\n",
    "                           columns=['sc_' + str(w) for w in NUM_COL])\n",
    "X_train = pd.concat([X_train, sc_num_cols], axis=1)\n",
    "\n",
    "sc_num_cols = pd.DataFrame(ss.transform(X_test[NUM_COL]), \n",
    "                           columns=['sc_' + str(w) for w in NUM_COL])\n",
    "X_test = pd.concat([X_test, sc_num_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.columns\n",
    "data = load_and_process_data()\n",
    "\n",
    "TGT_COL = ['income']\n",
    "NUM_COL = ['age', 'education-num', 'hours-per-week', 'capital-gain', 'capital-loss']\n",
    "CAT_COL = ['marital-status', 'relationship', 'race-cat', 'workclass-cat']\n",
    "\n",
    "dummies = pd.get_dummies(data[CAT_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_count = range(1,26)\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for nc in tqdm(neighbor_count):\n",
    "    knn = KNeighborsClassifier(n_neighbors=nc)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_acc.append(knn.score(X_train, y_train))\n",
    "    test_acc.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results = pd.DataFrame({\n",
    "    'N_Neighbors' : list(neighbor_count),\n",
    "    'Train_Acc' : train_acc,\n",
    "    'Test_Acc' : test_acc\n",
    "})\n",
    "\n",
    "knn_results.plot(x='N_Neighbors', title='Model Accuracy With More Neighbors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('Models/knn_cv.pkl'):\n",
    "    knn_cv = load_model('knn_cv')\n",
    "else: \n",
    "    param_grid = {\n",
    "        'n_neighbors' : neighbor_count\n",
    "    }\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_cv = GridSearchCV(knn, param_grid, cv=5)\n",
    "    knn_cv.fit(X_train, y_train)\n",
    "    save_model(knn_cv,'knn_cv')\n",
    "    \n",
    "print(f\"Tuned Logistic Regression Parameters: {knn_cv.best_params_}.\")\n",
    "print(f\"Best score is {knn_cv.best_score_}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_range = np.logspace(-5, 8, 15)\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for c in tqdm(c_range):\n",
    "    lr = LogisticRegression(C = c, solver='lbfgs')\n",
    "    lr.fit(X_train, y_train)\n",
    "    train_acc.append(lr.score(X_train, y_train))\n",
    "    test_acc.append(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = pd.DataFrame({\n",
    "    'Reg_Strength' : list(c_range),\n",
    "    'Train_Acc' : train_acc,\n",
    "    'Test_Acc' : test_acc\n",
    "})\n",
    "\n",
    "lr_results.plot(x='Reg_Strength', logx=True, title='Model Accuracy With Different Reg Strengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('Models/lr_cv.pkl'):\n",
    "    lr_cv = load_model('lr_cv')\n",
    "else:\n",
    "    param_grid = {\n",
    "        'C' : c_range\n",
    "    }\n",
    "\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=150)\n",
    "    lr_cv = GridSearchCV(lr, param_grid, cv=5)\n",
    "    lr_cv.fit(X_train, y_train)\n",
    "    save_model(lr_cv,'lr_cv')\n",
    "    \n",
    "print(f\"Tuned Logistic Regression Parameters: {lr_cv.best_params_}.\")\n",
    "print(f\"Best score is {lr_cv.best_score_}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Detour to Model Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = lr_cv.best_estimator_.coef_[0]\n",
    "importance_index = list(range(len(importance)))\n",
    "sns.barplot(x=importance_index, y=importance).set(title='LR Model Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at anything that has a coefficient of > 0.2\n",
    "importance_mask = np.abs(importance) > 0.2\n",
    "v_importance = importance[importance_mask]\n",
    "v_importance_names = X_train.columns[importance_mask]\n",
    "sns.barplot(x=v_importance_names, y=v_importance)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what we can get out of this:\n",
    "\n",
    "Benefit:\n",
    "- Married with spouse\n",
    "- Being a husband\n",
    "- Being older\n",
    "- More years of education\n",
    "- More hours per week\n",
    "\n",
    "Detractor:\n",
    "- Never being married\n",
    "- Not in a family\n",
    "- Having a child\n",
    "\n",
    "I find it very interesting that being a husband is important, but being male is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance[X_train.columns == 'sex']\n",
    "# Ah, so it is in there, and preferences males, but it was just a tad below the threshold I had decided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pc = pca.fit_transform(X_train)\n",
    "pca.explained_variance_ratio_.cumsum()\n",
    "# Wow, so 99.6% of the variance can be explained by one variable?  Thats crazy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth' : range(1,10),\n",
    "    'min_samples_split' : range(2,11),\n",
    "    'min_samples_leaf' : range(1,5)\n",
    "}\n",
    "\n",
    "dc = DecisionTreeClassifier()\n",
    "dc_cv = RandomizedSearchCV(dc, param_grid, cv=5, n_iter=20, scoring='accuracy')\n",
    "\n",
    "start = timer()\n",
    "dc_cv.fit(X_train, y_train)\n",
    "end = timer()\n",
    "\n",
    "print(f\"Best decision tree random grid search parameters: {dc_cv.best_params_}.\")\n",
    "print(f\"Best score is {dc_cv.best_score_}.\")\n",
    "print(f\"This took {end-start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was actually really fast.  \n",
    "# For giggles, lets see how long it takes to do the whole grid and see what the gain is\n",
    "\n",
    "if exists('Models/dc_cv.pkl'):\n",
    "    dc_cv = load_model('dc_cv')\n",
    "else:\n",
    "    dc_cv = GridSearchCV(dc, param_grid, cv=5)\n",
    "\n",
    "    start = timer()\n",
    "    dc_cv.fit(X_train, y_train)\n",
    "    end = timer()\n",
    "    print(f\"This took {end-start} seconds.\")\n",
    "    save_model(dc_cv,'dc_cv')\n",
    "\n",
    "print(f\"Best decision tree grid search parameters: {dc_cv.best_params_}.\")\n",
    "print(f\"Best score is {dc_cv.best_score_}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do one quick one to make sure everything is working as planned with default parameters.\n",
    "nn = MLPClassifier(learning_rate='adaptive', solver='adam', alpha=0.1) \n",
    "nn.fit(X_train, y_train)\n",
    "nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('Models/nn_cv.pkl'):\n",
    "    nn_cv = load_model('nn_cv')\n",
    "else:\n",
    "    nn = MLPClassifier(learning_rate='adaptive', max_iter=300)\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.01, 0.1],\n",
    "    }\n",
    "\n",
    "    nn_cv = RandomizedSearchCV(nn, param_grid, cv=5, scoring='accuracy', n_iter=10)\n",
    "\n",
    "    start = timer()\n",
    "    nn_cv.fit(X_train, y_train)\n",
    "    end = timer()\n",
    "    print(f\"This took {end-start} seconds.\")\n",
    "    save_model(nn_cv, 'nn_cv')\n",
    "\n",
    "print(f\"Best sklearn neural network grid search parameters: {nn_cv.best_params_}.\")\n",
    "print(f\"Best score is {nn_cv.best_score_}.\")\n",
    "\n",
    "#{'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (100,), 'alpha': 0.01, 'activation': 'tanh'}\n",
    "#Best score is 0.8513513513513513.\n",
    "#This took 1847.9064502009999 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets do some more voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('knn', knn_cv.best_estimator_),\n",
    "    ('lr', lr_cv.best_estimator_),\n",
    "    ('dc', dc_cv.best_estimator_),\n",
    "    ('nn', nn_cv.best_estimator_)\n",
    "]\n",
    "\n",
    "for i in range(2,len(models)+1):\n",
    "    for m in itertools.combinations(models, i):\n",
    "        models_being_looked_at = ', '.join([j[0] for j in m])\n",
    "        vc = VotingClassifier(m, voting='soft')\n",
    "        vc.fit(X_train, y_train)\n",
    "        print(f'Voting using: {models_being_looked_at} yielded an accuracy of {vc.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model came from using knn, dc, nn, so lets make a model with that.\n",
    "best_models = [\n",
    "    ('knn', knn_cv.best_estimator_),\n",
    "    ('dc', dc_cv.best_estimator_),\n",
    "    ('nn', nn_cv.best_estimator_)\n",
    "]\n",
    "\n",
    "if exists('Models/best_vc.pkl'):\n",
    "    best_vc = load_model('best_vc')\n",
    "else:\n",
    "    best_vc = VotingClassifier(best_models, voting='soft').fit(X_train, y_train)\n",
    "    save_model(best_vc, 'best_vc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about Stacking?\n",
    "\n",
    "https://www.geeksforgeeks.org/stacking-in-machine-learning-2/\n",
    "\n",
    "Stacking is a way of ensembling classification or regression models it consists of two-layer estimators. The first layer consists of all the baseline models that are used to predict the outputs on the test datasets. The second layer consists of Meta-Classifier or Regressor which takes all the predictions of baseline models as an input and generate new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'vc' not in [i[0] for i in models]:\n",
    "    models.append(('vc', best_vc))\n",
    "\n",
    "if exists('Models/lr_stack.pkl'):\n",
    "    lr_stack = load_model('lr_stack')\n",
    "else:\n",
    "    lr_stack = StackingClassifier(classifiers=[c[1] for c in models], \n",
    "                                   meta_classifier=LogisticRegression(max_iter=250, solver='lbfgs'), \n",
    "                                   use_probas=True, \n",
    "                                   use_features_in_secondary=True)\n",
    "    lr_stack.fit(X_train, y_train)\n",
    "    save_model(lr_stack, 'lr_stack')\n",
    "    \n",
    "if 'stack' not in [i[0] for i in models]:\n",
    "    models.append(('stack', lr_stack))\n",
    "\n",
    "print(f'Stacking yielded an accuracy of {lr_stack.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    y_pred = m[1].predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'Model {m[0]} has an RMSE of {rmse:.3f} and an accuracy of {accuracy_score(y_test, y_pred)*100:.3f}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost\n",
    "\n",
    "Gradient boosting re-defines boosting as a numerical optimization problem where the objective is to minimize the loss function of the model by adding weak learners using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(objective='reg:logistic', max_depth=5, seed=42)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "xgb_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth' : [3, 4, 5],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1],\n",
    "    'n_estimators' : [100, 200, 300]\n",
    "}\n",
    "\n",
    "if exists('Models/xgb_cv.pkl'):\n",
    "    xgb_cv = load_model('xgb_cv')\n",
    "else:\n",
    "    estimator = xgb.XGBClassifier(objective='reg:logistic')\n",
    "\n",
    "    xgb_cv = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    xgb_cv.fit(X_train, y_train)\n",
    "    save_model(xgb_cv, 'xgb_cv')\n",
    "    \n",
    "if 'xgb' not in [i[0] for i in models]:\n",
    "    models.append(('xgb', xgb_cv.best_estimator_))\n",
    "\n",
    "print(f\"Best XG Boost grid search parameters: {xgb_cv.best_params_}.\")\n",
    "print(f\"Best score is {xgb_cv.best_score_}.\")\n",
    "print(f'Model has an RMSE of {np.sqrt(mean_squared_error(xgb_cv.best_estimator_.predict(X_test), y_test))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = xgb_cv.best_estimator_.feature_importances_\n",
    "importance_index = list(range(len(importance)))\n",
    "sns.barplot(x=importance_index, y=importance).set(title='XGB Model Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at anything that has a coefficient of > 0.05\n",
    "# This is way more clustered than otherwise.\n",
    "importance_mask = np.abs(importance) > 0.05\n",
    "v_importance = importance[importance_mask]\n",
    "v_importance_names = X_train.columns[importance_mask]\n",
    "sns.barplot(x=v_importance_names, y=v_importance).set(title='XGB Model Importance >5%')\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "def bo_tune_xgb(max_depth, gamma, n_estimators, learning_rate):\n",
    "    params = {\n",
    "        'max_depth' : int(max_depth),\n",
    "        'gamma' : gamma,\n",
    "        'n_estimators' : int(n_estimators),\n",
    "        'learning_rate' : learning_rate,\n",
    "        'sub_sample' : 0.8,\n",
    "        'eta' : 0.1,\n",
    "        'eval_metric' : 'rmse'\n",
    "    }\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=70, nfold=5)\n",
    "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n",
    "\n",
    "xgb_bo = BayesianOptimization(\n",
    "    bo_tune_xgb,\n",
    "    {\n",
    "        'max_depth' : (3,10),\n",
    "        'gamma' : (0,1),\n",
    "        'learning_rate' : (0,1),\n",
    "        'n_estimators' : (100,200)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_bo.maximize(n_iter=5, init_points=8, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = xgb_bo.max['params']\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = int(params['max_depth'])\n",
    "params['n_estimators'] = int(params['n_estimators'])\n",
    "\n",
    "bae_xgb = xgb.XGBClassifier(**params, objective='reg:logistic').fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy score is {bae_xgb.score(X_test, y_test)}.\")\n",
    "print(f'Model has an RMSE of {np.sqrt(mean_squared_error(bae_xgb.predict(X_test), y_test))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "k_model = Sequential()\n",
    "k_model.add(Dense(100, activation='relu', input_shape=(X_test.shape[1],)))\n",
    "k_model.add(Dense(100, activation='relu'))\n",
    "k_model.add(Dense(1, activation='sigmoid'))\n",
    "k_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "k_model.fit(X_train, y_train, epochs=5)#, callbacks=[early_stopping_monitor])\n",
    "k_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pca and scaling: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "feature importance: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "pipeline: https://github.com/datacamp/course-resources-ml-with-experts-budgets/blob/master/notebooks/1.0-full-model.ipynb\n",
    "kaggle: https://www.kaggle.com/overload10/income-prediction-on-uci-adult-dataset\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
